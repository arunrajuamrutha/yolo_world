{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48e5cc78",
   "metadata": {},
   "source": [
    "YOLO-WORLD\n",
    "\n",
    "The YOLO-World Model introduces an advanced, real-time Ultralytics YOLOv8-based approach for Open-Vocabulary Detection tasks. This innovation enables the detection of any object within an image based on descriptive texts. By significantly lowering computational demands while preserving competitive performance, YOLO-World emerges as a versatile tool for numerous vision-based applications.\n",
    "\n",
    "YOLO-WORLD paper: https://arxiv.org/pdf/2401.17270v2.pdf\n",
    "\n",
    "Source code: https://github.com/AILab-CVC/YOLO-World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7e3eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0043312e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8.1.23'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ultralytics\n",
    "ultralytics.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3b5547",
   "metadata": {},
   "source": [
    "loading a pre-trained yolo-world model and running a prediction on images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6b4b84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8l-world.pt to 'yolov8l-world.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 91.2M/91.2M [00:05<00:00, 17.2MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/20 C:\\Users\\hp\\Desktop\\machine-test\\datasets\\val\\images\\RPReplay_Final1667001201_MP4-459_jpg.rf.352ca04a4c2cc5df8bf2dd3d8c01c120.jpg: 640x640 1 person, 1006.8ms\n",
      "image 2/20 C:\\Users\\hp\\Desktop\\machine-test\\datasets\\val\\images\\TsirinTu0018_jpg.rf.3fe2fc0fdb3a31434f1bac91d61e53be.jpg: 640x640 (no detections), 974.8ms\n",
      "image 3/20 C:\\Users\\hp\\Desktop\\machine-test\\datasets\\val\\images\\Video1_2_jpg.rf.0754a5fb79396bd903a4ded0145f7bc9.jpg: 640x640 1 person, 941.1ms\n",
      "image 4/20 C:\\Users\\hp\\Desktop\\machine-test\\datasets\\val\\images\\Video2_143_jpg.rf.7035ecddb4b2c8cfdbd5a004d946a496.jpg: 640x640 1 person, 1 bench, 985.2ms\n",
      "image 5/20 C:\\Users\\hp\\Desktop\\machine-test\\datasets\\val\\images\\Video2_150_jpg.rf.b9afc4a41d95211f77fb12a37629e211.jpg: 640x640 1 person, 2 benchs, 927.3ms\n",
      "image 6/20 C:\\Users\\hp\\Desktop\\machine-test\\datasets\\val\\images\\Video2_174_jpg.rf.d04a29e17bdf3c1135e2a0d6744250c0.jpg: 640x640 1 person, 1 bench, 942.8ms\n",
      "image 7/20 C:\\Users\\hp\\Desktop\\machine-test\\datasets\\val\\images\\Video2_178_jpg.rf.589896c6e0322de89e52bcd29313c422.jpg: 640x640 1 person, 1 bench, 988.1ms\n",
      "image 8/20 C:\\Users\\hp\\Desktop\\machine-test\\datasets\\val\\images\\Video2_38_jpg.rf.a593c2439bc2acd57f3d9e911c710e75.jpg: 640x640 1 person, 1 bench, 939.8ms\n",
      "image 9/20 C:\\Users\\hp\\Desktop\\machine-test\\datasets\\val\\images\\Video2_42_jpg.rf.f8ab7627f1a6ff7bcb89500ce01f949a.jpg: 640x640 1 person, 1 bench, 952.1ms\n",
      "image 10/20 C:\\Users\\hp\\Desktop\\machine-test\\datasets\\val\\images\\Video2_83_jpg.rf.3f002b5fe1217bb7937e1ac3eb8488ae.jpg: 640x640 1 person, 1 bus, 1 umbrella, 922.8ms\n",
      "image 11/20 C:\\Users\\hp\\Desktop\\machine-test\\datasets\\val\\images\\Video2_91_jpg.rf.faca99c76175b53a646f60aa1abc4b14.jpg: 640x640 1 person, 1 bench, 909.7ms\n",
      "image 12/20 C:\\Users\\hp\\Desktop\\machine-test\\datasets\\val\\images\\Video3_19_jpg.rf.ae0c1d1923ab81af7575d5d532490729.jpg: 640x640 1 person, 952.1ms\n",
      "image 13/20 C:\\Users\\hp\\Desktop\\machine-test\\datasets\\val\\images\\Video3_234_jpg.rf.471c24f2553d586d17a85abce416276b.jpg: 640x640 1 person, 942.4ms\n",
      "image 14/20 C:\\Users\\hp\\Desktop\\machine-test\\datasets\\val\\images\\Video3_42_jpg.rf.38bac29f7f62c6a04896444e6e26a925.jpg: 640x640 1 person, 1 airplane, 908.0ms\n",
      "image 15/20 C:\\Users\\hp\\Desktop\\machine-test\\datasets\\val\\images\\YouTube-FREE-AerialStockFootage_CityUrban-YKY3Mm5P1tE-720p_mp4-31_jpg.rf.c88b23a01eb99ef16b5208d11dd5764f.jpg: 640x640 1 bus, 961.5ms\n",
      "image 16/20 C:\\Users\\hp\\Desktop\\machine-test\\datasets\\val\\images\\YouTube-FREE-AerialStockFootage_CityUrban-YKY3Mm5P1tE-720p_mp4-7_jpg.rf.3ad1e4933e697179f9d54d06d31830e4.jpg: 640x640 (no detections), 933.0ms\n",
      "image 17/20 C:\\Users\\hp\\Desktop\\machine-test\\datasets\\val\\images\\YouTube-FreeStockFootage-PersonThinkingDeeply-h-HC-hj-Zo-720p_mp4-7_jpg.rf.4d6b125dc1fa9f9970e29a35bab61c9a.jpg: 640x640 1 person, 1 laptop, 922.9ms\n",
      "image 18/20 C:\\Users\\hp\\Desktop\\machine-test\\datasets\\val\\images\\YouTube-FreeStockFootage_Child-playing-with-parents-JoyLaughterHD-RQ_qqCZOkZk-720p_mp4-34_jpg.rf.01d6349eabce5613415c586a543c9f0b.jpg: 640x640 2 persons, 905.5ms\n",
      "image 19/20 C:\\Users\\hp\\Desktop\\machine-test\\datasets\\val\\images\\YouTube-FreeStockFootage_Child-playing-with-parents-JoyLaughterHD-RQ_qqCZOkZk-720p_mp4-35_jpg.rf.d03a2efbca9287a52d02a3d428d2aaf5.jpg: 640x640 2 persons, 1 couch, 1 book, 1 teddy bear, 958.6ms\n",
      "image 20/20 C:\\Users\\hp\\Desktop\\machine-test\\datasets\\val\\images\\youtube-6_jpg.rf.a9f31f242ee7d731c625ed07f6002b9b.jpg: 640x640 (no detections), 888.0ms\n",
      "Speed: 2.8ms preprocess, 943.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLOWorld\n",
    "\n",
    "# Initialize a YOLO-World model\n",
    "model = YOLOWorld('yolov8l-world.pt')  # or select yolov8m/l-world.pt for different sizes\n",
    "\n",
    "# Execute inference with the YOLOv8s-world model on the specified image\n",
    "results = model.predict('C:\\\\Users\\\\hp\\\\Desktop\\\\machine-test\\\\datasets\\\\val\\\\images', device='cpu', save=True)\n",
    "\n",
    "# Show results\n",
    "results[0].show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
