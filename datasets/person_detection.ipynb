{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8896656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8.1.23'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ultralytics\n",
    "ultralytics.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8404f958",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLOWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7d2625c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a YOLO-World model\n",
    "model = YOLOWorld('yolov8l-world.pt')  # or select yolov8m/l-world.pt for different sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f45636a",
   "metadata": {},
   "source": [
    "Set prompts for specific tasks# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7e59d2",
   "metadata": {},
   "source": [
    "The YOLO-World framework allows for the dynamic specification of classes through custom prompts, empowering users to tailor the model to their specific needs without retraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d28de902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['git+https://github.com/openai/CLIP.git'] not found, attempting AutoUpdate...\n",
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to c:\\users\\hp\\appdata\\local\\temp\\pip-req-build-vp6dbqy2\n",
      "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting ftfy (from clip==1.0)\n",
      "  Downloading ftfy-6.2.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: regex in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from clip==1.0) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from clip==1.0) (4.66.1)\n",
      "Requirement already satisfied: torch in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from clip==1.0) (2.1.2)\n",
      "Requirement already satisfied: torchvision in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from clip==1.0) (0.16.2)\n",
      "Collecting wcwidth<0.3.0,>=0.2.12 (from ftfy->clip==1.0)\n",
      "  Downloading wcwidth-0.2.13-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->clip==1.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from torch->clip==1.0) (4.5.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->clip==1.0) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->clip==1.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->clip==1.0) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->clip==1.0) (2024.2.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchvision->clip==1.0) (1.24.3)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchvision->clip==1.0) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchvision->clip==1.0) (9.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm->clip==1.0) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch->clip==1.0) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->torchvision->clip==1.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->torchvision->clip==1.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->torchvision->clip==1.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->torchvision->clip==1.0) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
      "Downloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n",
      "   ---------------------------------------- 54.4/54.4 kB 701.5 kB/s eta 0:00:00\n",
      "Downloading wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)\n",
      "Building wheels for collected packages: clip\n",
      "  Building wheel for clip (setup.py): started\n",
      "  Building wheel for clip (setup.py): finished with status 'done'\n",
      "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369505 sha256=7fe94da1d669e9d2ca4b629e466fe4b5cb881464942ad2d2e400cf284f01a163\n",
      "  Stored in directory: C:\\Users\\hp\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-hfel8jwv\\wheels\\3f\\7c\\a4\\9b490845988bf7a4db33674d52f709f088f64392063872eb9a\n",
      "Successfully built clip\n",
      "Installing collected packages: wcwidth, ftfy, clip\n",
      "  Attempting uninstall: wcwidth\n",
      "    Found existing installation: wcwidth 0.2.6\n",
      "    Uninstalling wcwidth-0.2.6:\n",
      "      Successfully uninstalled wcwidth-0.2.6\n",
      "Successfully installed clip-1.0 ftfy-6.2.0 wcwidth-0.2.13\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 10.7s, installed 1 package: ['git+https://github.com/openai/CLIP.git']\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 338M/338M [00:16<00:00, 21.2MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/20 C:\\Users\\hp\\Desktop\\machine-test\\datasets\\val\\images\\RPReplay_Final1667001201_MP4-459_jpg.rf.352ca04a4c2cc5df8bf2dd3d8c01c120.jpg: 640x640 2 persons, 1171.6ms\n",
      "image 2/20 C:\\Users\\hp\\Desktop\\machine-test\\datasets\\val\\images\\TsirinTu0018_jpg.rf.3fe2fc0fdb3a31434f1bac91d61e53be.jpg: 640x640 (no detections), 881.9ms\n",
      "image 3/20 C:\\Users\\hp\\Desktop\\machine-test\\datasets\\val\\images\\Video1_2_jpg.rf.0754a5fb79396bd903a4ded0145f7bc9.jpg: 640x640 1 person, 1024.1ms\n",
      "image 4/20 C:\\Users\\hp\\Desktop\\machine-test\\datasets\\val\\images\\Video2_143_jpg.rf.7035ecddb4b2c8cfdbd5a004d946a496.jpg: 640x640 1 person, 942.5ms\n",
      "image 5/20 C:\\Users\\hp\\Desktop\\machine-test\\datasets\\val\\images\\Video2_150_jpg.rf.b9afc4a41d95211f77fb12a37629e211.jpg: 640x640 1 person, 898.8ms\n",
      "image 6/20 C:\\Users\\hp\\Desktop\\machine-test\\datasets\\val\\images\\Video2_174_jpg.rf.d04a29e17bdf3c1135e2a0d6744250c0.jpg: 640x640 1 person, 1046.9ms\n",
      "image 7/20 C:\\Users\\hp\\Desktop\\machine-test\\datasets\\val\\images\\Video2_178_jpg.rf.589896c6e0322de89e52bcd29313c422.jpg: 640x640 1 person, 936.3ms\n",
      "image 8/20 C:\\Users\\hp\\Desktop\\machine-test\\datasets\\val\\images\\Video2_38_jpg.rf.a593c2439bc2acd57f3d9e911c710e75.jpg: 640x640 1 person, 909.7ms\n",
      "image 9/20 C:\\Users\\hp\\Desktop\\machine-test\\datasets\\val\\images\\Video2_42_jpg.rf.f8ab7627f1a6ff7bcb89500ce01f949a.jpg: 640x640 1 person, 937.1ms\n",
      "image 10/20 C:\\Users\\hp\\Desktop\\machine-test\\datasets\\val\\images\\Video2_83_jpg.rf.3f002b5fe1217bb7937e1ac3eb8488ae.jpg: 640x640 1 person, 904.1ms\n",
      "image 11/20 C:\\Users\\hp\\Desktop\\machine-test\\datasets\\val\\images\\Video2_91_jpg.rf.faca99c76175b53a646f60aa1abc4b14.jpg: 640x640 1 person, 962.7ms\n",
      "image 12/20 C:\\Users\\hp\\Desktop\\machine-test\\datasets\\val\\images\\Video3_19_jpg.rf.ae0c1d1923ab81af7575d5d532490729.jpg: 640x640 1 person, 901.2ms\n",
      "image 13/20 C:\\Users\\hp\\Desktop\\machine-test\\datasets\\val\\images\\Video3_234_jpg.rf.471c24f2553d586d17a85abce416276b.jpg: 640x640 1 person, 886.6ms\n",
      "image 14/20 C:\\Users\\hp\\Desktop\\machine-test\\datasets\\val\\images\\Video3_42_jpg.rf.38bac29f7f62c6a04896444e6e26a925.jpg: 640x640 1 person, 901.4ms\n",
      "image 15/20 C:\\Users\\hp\\Desktop\\machine-test\\datasets\\val\\images\\YouTube-FREE-AerialStockFootage_CityUrban-YKY3Mm5P1tE-720p_mp4-31_jpg.rf.c88b23a01eb99ef16b5208d11dd5764f.jpg: 640x640 (no detections), 891.0ms\n",
      "image 16/20 C:\\Users\\hp\\Desktop\\machine-test\\datasets\\val\\images\\YouTube-FREE-AerialStockFootage_CityUrban-YKY3Mm5P1tE-720p_mp4-7_jpg.rf.3ad1e4933e697179f9d54d06d31830e4.jpg: 640x640 (no detections), 894.5ms\n",
      "image 17/20 C:\\Users\\hp\\Desktop\\machine-test\\datasets\\val\\images\\YouTube-FreeStockFootage-PersonThinkingDeeply-h-HC-hj-Zo-720p_mp4-7_jpg.rf.4d6b125dc1fa9f9970e29a35bab61c9a.jpg: 640x640 1 person, 942.6ms\n",
      "image 18/20 C:\\Users\\hp\\Desktop\\machine-test\\datasets\\val\\images\\YouTube-FreeStockFootage_Child-playing-with-parents-JoyLaughterHD-RQ_qqCZOkZk-720p_mp4-34_jpg.rf.01d6349eabce5613415c586a543c9f0b.jpg: 640x640 2 persons, 992.7ms\n",
      "image 19/20 C:\\Users\\hp\\Desktop\\machine-test\\datasets\\val\\images\\YouTube-FreeStockFootage_Child-playing-with-parents-JoyLaughterHD-RQ_qqCZOkZk-720p_mp4-35_jpg.rf.d03a2efbca9287a52d02a3d428d2aaf5.jpg: 640x640 3 persons, 942.8ms\n",
      "image 20/20 C:\\Users\\hp\\Desktop\\machine-test\\datasets\\val\\images\\youtube-6_jpg.rf.a9f31f242ee7d731c625ed07f6002b9b.jpg: 640x640 (no detections), 947.7ms\n",
      "Speed: 3.5ms preprocess, 945.8ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Define custom classes\n",
    "model.set_classes([\"person\"])\n",
    "\n",
    "# Execute prediction for specified categories on an image\n",
    "results = model.predict('C:\\\\Users\\\\hp\\Desktop\\\\machine-test\\\\datasets\\\\val\\\\images', device='cpu', save=True)\n",
    "\n",
    "# Show results\n",
    "results[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b920c986",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
